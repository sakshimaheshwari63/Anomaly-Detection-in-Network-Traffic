{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT_pF75lbXIo",
        "outputId": "385bf20a-eae1-47f0-aeeb-b530f1cb2fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in file â†’ ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41']\n",
            "âš ï¸  No label column found â€“ running in **unsupervised** mode\n",
            "Isolation Forest flagged 31084 / 311029 records as anomalies\n",
            "Epoch 1/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.5926 - val_loss: 0.3111\n",
            "Epoch 2/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3860 - val_loss: 0.2430\n",
            "Epoch 3/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1963 - val_loss: 0.1832\n",
            "Epoch 4/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1542 - val_loss: 0.1554\n",
            "Epoch 5/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0996 - val_loss: 0.1375\n",
            "Epoch 6/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1137 - val_loss: 0.0988\n",
            "Epoch 7/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0802 - val_loss: 0.0866\n",
            "Epoch 8/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0765 - val_loss: 0.1116\n",
            "Epoch 9/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.1071 - val_loss: 0.0644\n",
            "Epoch 10/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0712 - val_loss: 0.0663\n",
            "Epoch 11/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0748 - val_loss: 0.0576\n",
            "Epoch 12/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0681 - val_loss: 0.0591\n",
            "Epoch 13/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.0514 - val_loss: 0.0670\n",
            "Epoch 14/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0736 - val_loss: 0.0589\n",
            "Epoch 15/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0567 - val_loss: 0.0604\n",
            "Epoch 16/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0444 - val_loss: 0.0699\n",
            "Epoch 17/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0533 - val_loss: 0.0535\n",
            "Epoch 18/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0511\n",
            "Epoch 19/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0407 - val_loss: 0.0442\n",
            "Epoch 20/20\n",
            "\u001b[1m972/972\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0477\n",
            "\u001b[1m9720/9720\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step\n",
            "Autoencoder flagged 15552 / 311029 records as anomalies (threshold=0.0807)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# --- 1. Load -----------------------------------------------------------------\n",
        "df = pd.read_csv(\"/content/kddcup_converted.csv\")      # change path as needed\n",
        "print(\"Columns in file â†’\", list(df.columns))   # <-- see what you actually have\n",
        "\n",
        "# --- 2. Identify the label column (if any) -----------------------------------\n",
        "possible_labels = [\"label\", \"class\", \"attack\", \"target\"]\n",
        "label_col = next((c for c in possible_labels if c in df.columns), None)\n",
        "\n",
        "if label_col:\n",
        "    print(f\"ğŸ‘‰ Using '{label_col}' as the groundâ€‘truth label column\")\n",
        "    y = df[label_col].copy()\n",
        "    # example binarization: 0â€¯=â€¯normal, 1â€¯=â€¯anomaly\n",
        "    if y.dtype == \"object\":\n",
        "        y = y.apply(lambda x: 0 if x.lower().startswith(\"normal\") else 1)\n",
        "    df = df.drop(columns=[label_col])\n",
        "else:\n",
        "    print(\"âš ï¸  No label column found â€“ running in **unsupervised** mode\")\n",
        "    y = None  # weâ€™ll only predict, not score\n",
        "\n",
        "# --- 3. Encode categoricals ---------------------------------------------------\n",
        "cat_cols = df.select_dtypes(include=\"object\").columns\n",
        "encoders = {col: LabelEncoder().fit(df[col]) for col in cat_cols}\n",
        "for col, enc in encoders.items():\n",
        "    df[col] = enc.transform(df[col])\n",
        "\n",
        "# --- 4. Scale -----------------------------------------------------------------\n",
        "X = df.values\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# --- 5. Isolation Forest ------------------------------------------------------\n",
        "iso = IsolationForest(contamination=0.1, random_state=42)\n",
        "pred_if = (iso.fit_predict(X_scaled) == -1).astype(int)\n",
        "\n",
        "if y is not None:\n",
        "    print(\"\\nIsolation Forest results\")\n",
        "    print(classification_report(y, pred_if, target_names=[\"Normal\", \"Anomaly\"]))\n",
        "else:\n",
        "    # just show how many points were flagged\n",
        "    print(f\"Isolation Forest flagged {pred_if.sum()} / {len(pred_if)} records as anomalies\")\n",
        "\n",
        "# --- 6. Autoencoder -----------------------------------------------------------\n",
        "input_dim = X.shape[1]\n",
        "auto = Sequential([\n",
        "    Input(shape=(input_dim,)),\n",
        "    Dense(32, activation=\"relu\"),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dense(32, activation=\"relu\"),\n",
        "    Dense(input_dim, activation=\"linear\"),\n",
        "])\n",
        "auto.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "# train only on data assumed normal\n",
        "train_mask = (y == 0) if y is not None else np.ones(len(X_scaled), dtype=bool)\n",
        "X_train, X_val = train_test_split(X_scaled[train_mask], test_size=0.2, random_state=42)\n",
        "\n",
        "auto.fit(X_train, X_train,\n",
        "         epochs=20, batch_size=256,\n",
        "         validation_data=(X_val, X_val), verbose=1)\n",
        "\n",
        "recons = auto.predict(X_scaled)\n",
        "mse = np.mean(np.square(X_scaled - recons), axis=1)\n",
        "thresh = np.percentile(mse, 95)\n",
        "pred_ae = (mse > thresh).astype(int)\n",
        "\n",
        "if y is not None:\n",
        "    print(\"\\nAutoencoder results\")\n",
        "    print(classification_report(y, pred_ae, target_names=[\"Normal\", \"Anomaly\"]))\n",
        "else:\n",
        "    print(f\"Autoencoder flagged {pred_ae.sum()} / {len(pred_ae)} records as anomalies \"\n",
        "          f\"(threshold={thresh:.4f})\")\n"
      ]
    }
  ]
}